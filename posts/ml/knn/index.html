<!DOCTYPE html>
<html lang="en" >
<head>
  <meta charset="utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>

  
  <meta name="author"
        content=""/>

  
  <meta name="description" content="An article about some basic concepts for KNN"/>
  

  

  
  <link rel="canonical" href="http://www.tczhong.com/posts/ml/knn/"/>

  

  <title>K Nearest Neighbor, K-Means and EM algorithm &middot; TC blog</title>

  <link rel="shortcut icon" href="http://www.tczhong.com/images/favicon.ico"/>
  <link rel="stylesheet" href="http://www.tczhong.com/css/animate.min.css"/>
  <link rel="stylesheet" href="http://www.tczhong.com/css/remixicon.css"/>
  <link rel="stylesheet" href="http://www.tczhong.com/css/zozo.css"/>
  <link rel="stylesheet" href="http://www.tczhong.com/css/highlight.css"/>

  
  
</head>

<body>
<div class="main animated">
  <div class="nav_container animated fadeInDown">
  <div class="site_nav" id="site_nav">
    <ul>
      
      <li>
        <a href="http://www.tczhong.com/posts/">Posts</a>
      </li>
      
      <li>
        <a href="http://www.tczhong.com/about/hello">About</a>
      </li>
      
      <li>
        <a href="http://www.tczhong.com/tianchen_zhong.pdf">My Resume</a>
      </li>
      
      <li>
        <a href="http://www.tczhong.com/posts/project/index.html">Project</a>
      </li>
      
    </ul>
  </div>
  <div class="menu_icon">
    <a id="menu_icon"><i class="remixicon-links-line"></i></a>
  </div>
</div>

  <div class="header animated fadeInDown">
  <div class="site_title_container">
    <div class="site_title">
      <h1>
        <a href="http://www.tczhong.com/">
          <span>TC blog</span>
          <img src="http://www.tczhong.com/img/kirby.png"/>
        </a>
      </h1>
    </div>
    <div class="description">
      <p class="sub_title"></p>
      <div class="my_socials">
        
        <a href="http://www.tczhong.com/index.xml" type="application/rss+xml" title="rss" target="_blank"><i class="remixicon-rss-fill"></i></a>
      </div>
    </div>
  </div>
</div>

  <div class="content">
    <div class="post_page">
      <div class="post animated fadeInDown">
        <div class="post_title post_detail_title">
          <h2><a href='http://www.tczhong.com/posts/ml/knn/'>K Nearest Neighbor, K-Means and EM algorithm</a></h2>
          <span class="date">2018.12.01</span>
        </div>
        <div class="post_content markdown"><h1 id="k-nearest-neighbor-k-means-and-em-algorithm">K Nearest Neighbor, K-Means and EM algorithm</h1>
<p>These three algorithms are used for clustering. Let&rsquo;s talk about K Nearest Neighbor(KNN) first.</p>
<h2 id="k-nearest-neighbor">K Nearest Neighbor</h2>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/320px-KnnClassification.svg.png?1546962079122" alt=""></p>
<p>Example of k-NN classification. The test sample (green circle) should be classified either to the first class of blue squares or to the second class of red triangles. If k = 3 (solid line circle) it is assigned to the second class because there are 2 triangles and only 1 square inside the inner circle. If k = 5 (dashed line circle) it is assigned to the first class (3 squares vs. 2 triangles inside the outer circle).</p>
<p>In edger learning, it will choose a hypothesis space H, and it is the hard bias. Then given the training data, it can find a member of H, h, that best fits in it. Then h could be applied to any data.</p>
<p>The lazy learning is doing nothing in training step, just saving all data. In test step, it will make a prediction based on data in this step.</p>
<p>For K-nearest Neighbors, it finds K nearest neighbors in the training data. Then average the label or find the most labels to predict this data&rsquo;s label. If K=1, it always finds itself. There is no hard bias for KNN, and the soft bias is nearby point should have the same label.</p>
<p>The problem for KNN is: it is possible that different dimension will have different meaning, it is not wise just treat them as same. We could use $w _ i$ to deal with different dimension problem.$f(q) = \frac{\sum K(dist(x _ i,q))y _ i}{\sum K(dist(x _ i,q))}$</p>
<p>The kernel function is any positive semidefinite function of any two inputs. We could use the kernel to estimate small dataset and aggregate them to estimate the whole dataset. We can also use kernel smoothing.</p>
<h2 id="k-means">K-Means</h2>
<p>In the beginning, pre-define K centers in the observation.</p>
<p>It contains two steps, and the first one is to assign the center of the cluster. Assign each observation to the cluster whose mean has the least squared Euclidean distance. The second one is to update the new center which is the mean of the observations in the new cluster. Iterate many times, until the centers will not change.</p>
<p>The algorithm does not guarantee convergence to the global optimum. The result may depend on the initial clusters. As the algorithm is usually fast, it is common to run it multiple times with different starting conditions.</p>
<p>![](<a href="https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/K-means">https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/K-means</a> _ convergence.gif/440px-K-means _ convergence.gif)</p>
<h2 id="em-algorithm">EM algorithm</h2>
<p>The EM algorithm is a general idea to calculate the unknown parameters. Let&rsquo;s use Gaussian Mixture Models as an example to talk about this model.</p>
<p>If there are many gaussian models mixing, we could get sampled data from this model. How could we estimate the $\mu _ i$ for these value?</p>
<p>Follow the MLE idea, we could get likelihood function with $\prod _ {i} e^{-\frac{(x _ {i}-\mu _ {j})^{2}}{2\sigma^{2}}}$ with $\mu _ j$. If we consider the prior $\lambda _ j$ for model j. The most possible for $\mu$ is
$$\boldsymbol{\mu} _ {ML}=\arg\max _ {\mu _ {1},\mu _ {2}}\prod _ {i}\sum _ {j}\lambda _ {j}e^{-\frac{(x _ {i}-\mu _ {j})^{2}}{2\sigma^{2}}}$$. But there is no analytical solution for this function, EM algorithm could give a local maximum, not necessary to be global maximum.</p>
<p>If we could get a table Z, when $x _ i$ from Gaussian j,$Z _ {i,j}=1$,otherwise 0. The $\hat \mu _ j = \sum _ i Z _ {i,j}X _ i / \sum Z _ {i,j}$. If we don&rsquo;t know it, we could estimate it with $\hat \mu _ j = \sum E[Z _ {i,j}|\bar \mu] X _ i / \sum E[Z _ {i,j}|\bar \mu]$.</p>
<p>The basic idea for EM is:</p>
<ol>
<li>Assume the $\mu _ j$ in the model</li>
<li>Classify data point to each model</li>
<li>Re-estimate the $\mu _ j$</li>
<li>Repeat</li>
</ol>
<p>In GMM:</p>
<ol>
<li>E steps: $$E \left[z _ {i,j}|\mu^{[k]}\right]=\frac{\lambda _ {j}L\left(x _ {i}|\mu=\mu _ {j}^{[k]}\right)}{\sum _ {j^{\prime}}\lambda _ {j^{\prime}}L\left(x _ {i}|\mu=\mu _ {j^{\prime}}^{[k]}\right)}$$
In this step, it could classify data point to each cluster.</li>
<li>M step: $$\mu _ {j}^{[k+1]}=\frac{\sum _ {i}E\left[z _ {i,j}|\mu^{[k]}\right]\cdot x _ {i}}{\sum _ {i}E\left[z _ {i,j}|\mu^{[k]}\right]}$$ In this step, it could re-estimate the $\mu _ j$</li>
</ol>
<p>if L is bounded, then it can converge to a (not necessarily global) local minimum.</p>
<p>We could see the KMeans algorithm has the similar idea with EM algorithm.</p>
</div>
        <div class="post_footer">
          
        </div>
      </div>
      
      
    </div>
  </div>
  <a id="back_to_top" href="#" class="back_to_top"><span>â–³</span></a>
</div>
<footer class="footer">
  <div class="powered_by">
    <a href="https://zeuk.me">Designed by Zeuk,</a>
    <a href="http://www.gohugo.io/">Proudly published with Hugo</a>
  </div>

  <div class="footer_slogan">
    <span></span>
  </div>
</footer>



<script src="http://www.tczhong.com/js/jquery-3.3.1.min.js"></script>
<script src="http://www.tczhong.com/js/zozo.js"></script>
<script src="http://www.tczhong.com/js/highlight.pack.js"></script>
<link  href="http://www.tczhong.com/css/fancybox.min.css" rel="stylesheet">
<script src="http://www.tczhong.com/js/fancybox.min.js"></script>

<script>hljs.initHighlightingOnLoad()</script>





<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "tczhong" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

</body>
</html>
