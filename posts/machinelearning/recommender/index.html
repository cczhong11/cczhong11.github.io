<!DOCTYPE html>
<html lang="en" >
<head>
  <meta charset="utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>

  
  <meta name="author"
        content=""/>

  
  <meta name="description" content="Basic idea for Recommendation"/>
  

  

  
  <link rel="canonical" href="http://www.tczhong.com/posts/machinelearning/recommender/"/>

  

  <title>Recommendation &middot; TC blog</title>

  <link rel="shortcut icon" href="http://www.tczhong.com/images/favicon.ico"/>
  <link rel="stylesheet" href="http://www.tczhong.com/css/animate.min.css"/>
  <link rel="stylesheet" href="http://www.tczhong.com/css/remixicon.css"/>
  <link rel="stylesheet" href="http://www.tczhong.com/css/zozo.css"/>
  <link rel="stylesheet" href="http://www.tczhong.com/css/highlight.css"/>

  
  
</head>

<body>
<div class="main animated">
  <div class="nav_container animated fadeInDown">
  <div class="site_nav" id="site_nav">
    <ul>
      
      <li>
        <a href="http://www.tczhong.com/posts/">Posts</a>
      </li>
      
      <li>
        <a href="http://www.tczhong.com/about/hello">About</a>
      </li>
      
      <li>
        <a href="http://www.tczhong.com/tianchen_zhong.pdf">My Resume</a>
      </li>
      
      <li>
        <a href="http://www.tczhong.com/posts/project/index.html">Project</a>
      </li>
      
    </ul>
  </div>
  <div class="menu_icon">
    <a id="menu_icon"><i class="remixicon-links-line"></i></a>
  </div>
</div>

  <div class="header animated fadeInDown">
  <div class="site_title_container">
    <div class="site_title">
      <h1>
        <a href="http://www.tczhong.com/">
          <span>TC blog</span>
          <img src="http://www.tczhong.com/img/kirby.png"/>
        </a>
      </h1>
    </div>
    <div class="description">
      <p class="sub_title"></p>
      <div class="my_socials">
        
        <a href="http://www.tczhong.com/index.xml" type="application/rss+xml" title="rss" target="_blank"><i class="remixicon-rss-fill"></i></a>
      </div>
    </div>
  </div>
</div>

  <div class="content">
    <div class="post_page">
      <div class="post animated fadeInDown">
        <div class="post_title post_detail_title">
          <h2><a href='http://www.tczhong.com/posts/machinelearning/recommender/'>Recommendation</a></h2>
          <span class="date">2018.05.01</span>
        </div>
        <div class="post_content markdown"><h1 id="recommendation">Recommendation</h1>
<h2 id="collaborative-filtering">Collaborative filtering</h2>
<p>Solely upon the preferences that other users have indicated for these items.(rating for items): row -&gt; users  column-&gt; items</p>
<p>The task of collaborative filtering, then, is to “fill in” the remaining entries of this matrix given the observed matrix. This X matrix that we observed is sparse but the unknown entries do not correspond to actual zeros in the matrix, but are rather just truly unknown.</p>
<p><img src="pic/matriex.png" alt=""></p>
<p>▪	User-user approaches: In this approach we estimate a user’s rating of an item by finding “similar” users and then looking at their predictions for this item.
▪	Item-item approaches: These methods take the converse approach, and estimate a user’s rating of a item by finding similar items and then looking at the user’s rating of these similar items.
▪	Matrix factorization: Finally, the last class of approaches works a little bit differently, by aiming to construct a low-rank matrix that approximates the observed entries of the rating matrix.</p>
<h2 id="user-user-approaches">user-user approaches</h2>
<p>To start, let&rsquo;s introduce a slightly more formal bit of notation to define our problem.  Let $\hat{X}<em>{ij}$ denote our prediction for the $i$th user and $j$th item (i.e., this will be one of the elements that is missing from the matrix $X$, which we want to predict).  A common form for the prediction make by the user-user approach would be
$$
\hat{X}</em>{ij} = \bar{x}<em>i + \frac{\sum</em>{k:X_{kj} \neq 0} w_{ik} (X_{kj} - \bar{x}_k)}{\sum_{k:X_{kj} \neq 0} \lvert w_{ik} \rvert}
$$
where $\bar{x}_i$ denotes the average of the observed ratings for user $i$, and $w_{ik}$ denotes a _similarity weight_ between user $i$ and user $k$ (which we will define shortly).  The intuition behind this approach is the following: if we want to predict user $i$'s rating for item $j$, we look across all users that _do_ have ratings for item $j$, and we average these together, weighted by a similarity function between the two users (we divide by $\sum_{k:X_{kj} \neq 0} \lvert w_{ik} \rvert$ so that we are taking a weighted average, noting that we take the absolute value because similarity weights can sometimes be positive or negative depending how we define then).  Because user&rsquo;s also frequently have their own &ldquo;baseline&rdquo; rating (i.e., some users naturally assign lower ratings than others), it&rsquo;s slightly better to do this modeling in the &ldquo;difference space&rdquo;, the difference between a user&rsquo;s rating and their mean rating, and then add re-scale by adding a user&rsquo;s mean score.</p>
<p>Let&rsquo;s see how this works in code.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict_user_user</span>(X, W, user_means, i, j):
    <span style="color:#e6db74">&#34;&#34;&#34; Return prediction of X_(ij). &#34;&#34;&#34;</span>
    <span style="color:#66d9ef">return</span> user_means[i] <span style="color:#f92672">+</span> (np<span style="color:#f92672">.</span>sum((X[:,j] <span style="color:#f92672">-</span> user_means) <span style="color:#f92672">*</span> (X[:,j] <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>) <span style="color:#f92672">*</span> W[i,:]) <span style="color:#f92672">/</span> 
                            np<span style="color:#f92672">.</span>sum((X[:,j] <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>abs(W[i,:])))
</code></pre></div><p>Item-item same as user-user instead of doing on columns</p>
<p>Weight could calculated by Pearson correlation. Let’s take the standard example of Pearson correlation (one of the most common methods for defining these weights)</p>
<p>$$
W_{ik} = \frac{\sum_{j \in \mathcal{I}_{ij}} (X_{ij} - \bar{x}_i)(X_{kj} - \bar{x}_k)}
{\sqrt{\sum_{j \in \mathcal{I}_{ij}}(X_{ij} - \bar{x}_i)^2} \sqrt{\sum_{j \in \mathcal{I}_{ij}}(X_{kj} - \bar{x}_k)^2}}
$$</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">pearson</span>(X,user_means, i,j):
    I <span style="color:#f92672">=</span> (X[i,:]<span style="color:#f92672">!=</span><span style="color:#ae81ff">0</span>) <span style="color:#f92672">*</span> (X[j,:]<span style="color:#f92672">!=</span><span style="color:#ae81ff">0</span>)
    xi <span style="color:#f92672">=</span> X[i,I] <span style="color:#f92672">-</span> user_means[i]
    xj <span style="color:#f92672">=</span> X[j,I] <span style="color:#f92672">-</span> user_means[j]
    <span style="color:#66d9ef">return</span> (xi <span style="color:#960050;background-color:#1e0010">@</span> xj)<span style="color:#f92672">/</span>(np<span style="color:#f92672">.</span>sqrt((xi <span style="color:#960050;background-color:#1e0010">@</span> xi)<span style="color:#f92672">*</span>(xj <span style="color:#960050;background-color:#1e0010">@</span> xj))<span style="color:#f92672">+</span><span style="color:#ae81ff">1e-12</span>)

</code></pre></div><h2 id="matrix-factorization">Matrix factorization</h2>
<p>Using this approach, we tried to find two vectors $X \approx \hat{X} = UV, ;; U \in \mathbb{R}^{m \times k}, ; V \in \mathbb{R}^{k \times n}$</p>
<h3 id="hypothesis-function">hypothesis function</h3>
<p>$\hat{X}<em>{ij} \equiv h</em>\theta(i,j) = u_i^T v_j$</p>
<p>our parameters are just all the $u$ and $v$ vectors, $\theta = {u_{1:m}, v_{1:n}}$.  One way to interpret this is that you can think of $u_i$ and $v_j$ as being something that is _both_ like a feature vector and a parameter vector.  For a given user $i$, our hypothesis function is a linear hypothesis with paramters $u_i$, and we make our predictions by taking the inner product with these parameters and the item &ldquo;features&rdquo; $v_j$.  Thus, the goal of matrix factorization is to simultaneously learn both the per-user coefficients and the per-item features.</p>
<h3 id="loss-function">loss function</h3>
<p>$\ell(h_\theta(i,j), X_{ij}) = (h_\theta(i,j) - X_{ij})^2$</p>
<h3 id="optimization-problem">optimization problem</h3>
<p>$minimize_{u_{1:m},v_{1:n}} \sum_{i,j \in S} (u_i^T v_j - X_{ij})^2.$</p>
<h3 id="alternative-least-square">alternative least square</h3>
<p>In this method, we will fix one vector and calculate another vector to make sure that one has the minimum error rate.</p>
<p>$$
u_i = \left ( \sum_{j : (i,j) \in S} v_j v_j^T \right )^{-1} \left (\sum_{j : (i,j) \in S} v_j X_ij \right ), ;; \ i=1,\ldots,m
$$</p>
<p>$$
v_j = \left ( \sum_{i : (i,j) \in S} u_i u_i^T \right )^{-1} \left (\sum_{i : (i,j) \in S} u_i X_ij \right ), ;; j=1,\ldots,n
$$</p>
<h3 id="relation-to-pca">relation to PCA</h3>
<p>As mentioned above, there is a close relationship between matrix factorization for collaborative filtering and PCA.  Both are finding low-rank approximation to some matrix $X$.  But the key difference is that while <strong>PCA tries to find an approximation that matches <em>all</em> the entries of $X$</strong> (that is, $S$ would consist of the set of all valid $i,j$ pairs), matrix factorization for collaborative filtering only considers the loss on the observed entries.  Although we won&rsquo;t get into the specifics here, it turns out that this difference means that PCA can be solved optimally an eigenvalue decomposition (or equivalently, a singular value decomposition), whereas matrix factorization cannot be solved in this analytical manner, and the alternating optimization scheme we mentioned above has the potential for local optima.</p>
<p>Because of this, <strong>it is somewhat common to initialize matrix factorization with $u$ and $v$ terms determined by PCA</strong> (probably subtracting the mean of the data first as in typical PCA, so we don&rsquo;t try too hard <em>too</em> hard to fit the zero entries).  Doing so is not required by any means, but it is a nice way of proving a non-random initial solution to the problem, so that we can begin the matrix factorization steps.</p>
</div>
        <div class="post_footer">
          
        </div>
      </div>
      
      
    </div>
  </div>
  <a id="back_to_top" href="#" class="back_to_top"><span>△</span></a>
</div>
<footer class="footer">
  <div class="powered_by">
    <a href="https://zeuk.me">Designed by Zeuk,</a>
    <a href="http://www.gohugo.io/">Proudly published with Hugo</a>
  </div>

  <div class="footer_slogan">
    <span></span>
  </div>
</footer>



<script src="http://www.tczhong.com/js/jquery-3.3.1.min.js"></script>
<script src="http://www.tczhong.com/js/zozo.js"></script>
<script src="http://www.tczhong.com/js/highlight.pack.js"></script>
<link  href="http://www.tczhong.com/css/fancybox.min.css" rel="stylesheet">
<script src="http://www.tczhong.com/js/fancybox.min.js"></script>

<script>hljs.initHighlightingOnLoad()</script>





<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "tczhong" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

</body>
</html>
