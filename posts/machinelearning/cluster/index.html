<!DOCTYPE html>
<html lang="en">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.89.4" />

    
    
    

<title>Clustering • TC blog</title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Clustering"/>
<meta name="twitter:description" content="This blog tells you do cluster"/>

<meta property="og:title" content="Clustering" />
<meta property="og:description" content="This blog tells you do cluster" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://www.tczhong.com/posts/machinelearning/cluster/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-05-01T00:00:00+00:00" />
<meta property="article:modified_time" content="2018-05-01T00:00:00+00:00" /><meta property="og:site_name" content="TC blog" />



    


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/vs.min.css">








<link rel="stylesheet" href="http://www.tczhong.com/scss/hyde-hyde.3081c4981fb69a2783dd36ecfdd0e6ba7a158d4cbfdd290ebce8f78ba0469fc6.css" integrity="sha256-MIHEmB&#43;2mieD3Tbs/dDmunoVjUy/3SkOvOj3i6BGn8Y=">


<link rel="stylesheet" href="http://www.tczhong.com/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">



    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://www.tczhong.com/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="http://www.tczhong.com/favicon.png">
    
    

</head>


    <body class=" ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <span class="site__title">
        <a href="http://www.tczhong.com/">TC blog</a>
      </span>
      
        
        
        
        <div class="author-image">
          <img src="https://avatars2.githubusercontent.com/u/7358252?s=460&amp;v=4" alt="Author Image" class="img--circle img--headshot element--center">
        </div>
        
      
      
      <p class="site__description">
         I am who I am. Love Cloud and Data science 
      </p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">TC blog</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="http://www.tczhong.com/posts/">
						<span>Posts</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="http://www.tczhong.com/about/hello">
						<span>About</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="http://www.tczhong.com/tianchen_zhong.pdf">
						<span>My Resume</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="http://www.tczhong.com/posts/project/index.html">
						<span>Project</span>
					</a>
				</li>
			 
		
	</ul>
</div>

        <section class="social">
	
	
	
	<a href="https://github.com/cczhong11" rel="me"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	<a href="https://linkedin.com/in/tianchen-zhong" rel="me"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	<a href="https://stackoverflow.com/users/7112540" rel="me"><i class="fab fa-stack-overflow fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	
	<a href="mailto:tczhong24@gmail.com" rel="me"><i class="fas fa-at fa-lg" aria-hidden="true"></i></a>
	
</section>

      </div>
    </div>
    


  </div>
</div>

        <div class="content container">
            
    
<article>
  <header>
    <h1>Clustering</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> May 5, 2018
    
    
    
      
      
          in
          
          
              <a class="badge badge-category" href="http://www.tczhong.com/categories/machine-learning">MACHINE LEARNING</a>
              
          
      
    
    
    
    <br/>
    <i class="fas fa-clock"></i> 4 min read
</div>


  </header>
  
  
  <div class="post">
    <h1 id="unsupervised-learning">unsupervised learning</h1>
<p>the general philosophy of unsupervised learning is that we want to discover some kind of structure in the data. Different unsupervised learning methods work in very different ways, and discover very different kinds of structure, but they all have this similar element.</p>
<p>Recall from our presentations on supervised learnign that the three aspects of a supervised learning algorithm are:</p>
<ol>
<li>a hypothesis function;($R^n -&gt; R^n$). Mapping input back to this input <strong>space.The goal of this hypothesis class is to approximately reconstruct the input</strong></li>
<li>a loss function; the goal of an unsupervised learning loss function is to measure the <strong>difference between the hypothesis function and the input itself</strong>.</li>
<li>a method for minimizing the average loss over the training data. in the unsupervised setting there (just empirically) is a bit more <strong>variation</strong> in the methods people use to solve this optimization problem. This is because the hypothesis functions themselves often involve discrete terms or other similar elements that cannot be differentiated (we’ll see this is the case of k-means clustering), or because particular optimization methods can provide exact solution</li>
</ol>
<h1 id="kmeans">kmeans</h1>
<h2 id="hypothesis-function">Hypothesis function</h2>
<p>The parameters $\theta$ of our hypothesis function just include the centers themselves.
$$\theta = {\mu^{(1)}, \ldots, \mu^{(k)}}$$</p>
<p>The function is $$h_\theta(x) = argmin_{\mu \in {\mu^{(1)}, \ldots, \mu^{(k)}} } |\mu - x|_2^2$$</p>
<h2 id="loss-function">Loss function</h2>
<p>The loss function used by k-means is simply the squared loss
$$\ell(h_\theta(x), x) = \min_{\mu \in {\mu^{(1)}, \ldots, \mu^{(k)}}} |\mu - x|_2^2$$</p>
<h2 id="optimization">Optimization</h2>
<p>Finally, let&rsquo;s now consider the ML optimization problem that results from the hypothesis and loss above,
$$minimize_\theta ;; \frac{1}{m} \sum_{i=1}^m \min_{\mu \in {\mu^{(1)}, \ldots, \mu^{(k)}}} |\mu - x^{(i)}|_2^2.$$</p>
<ol>
<li>choose centre randomly</li>
<li>repeat until convergence</li>
<li>for every i, set $c^{(i)}=argmin ||x^{(i)}-\mu_j||^2$ $c^{(i)}$ is label for each point</li>
<li>for every $\mu_j$ set $\mu_j=\frac{\sum (c(i)=j)x^{i}}{\sum (c(i)=j)}$ Each centroid is the geometric mean of the points that have that centroid&rsquo;s label. Important: If a centroid is empty</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">kmeans</span>(X, k, max_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, rand_seed<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>):
    np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(rand_seed)
    Mu <span style="color:#f92672">=</span> X[np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>],k),:]
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(max_iter):
        D <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>X<span style="color:#a6e22e">@Mu</span><span style="color:#f92672">.</span>T <span style="color:#f92672">+</span> (X<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)[:,<span style="color:#66d9ef">None</span>] <span style="color:#f92672">+</span> (Mu<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
        y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmin(D,axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
        Mu <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([np<span style="color:#f92672">.</span>mean(X[y<span style="color:#f92672">==</span>i],axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(k)])
    loss <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>norm(X <span style="color:#f92672">-</span> Mu[np<span style="color:#f92672">.</span>argmin(D,axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),:])<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span><span style="color:#f92672">/</span>X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
    <span style="color:#66d9ef">return</span> Mu, y, loss
</code></pre></div><p>The problem for k-means is it is possible to get into local optima.</p>
<h1 id="expectation-maximization">Expectation Maximization</h1>
<p>the EM algorithm gives an efficient method for maximum likelihood estimation. Maximizing ℓ(θ) explicitly might be difficult, and our strategy will be to instead repeatedly construct a lower-bound on l(E-step), and then optimize that lower-bound (M-step).</p>
<p>Lets review EM. In EM, you randomly initialize your model parameters, then you alternate between (E) assigning values to hidden variables, based on parameters and (M) computing parameters based on fully observed data.</p>
<p><strong>E-Step</strong>: Coming up with values to hidden variables, based on parameters. If you work out the math of chosing the best values for the class variable based on the features of a given piece of data in your data set, it comes out to &ldquo;<strong>for each data-point, chose the centroid that it is closest to, by euclidean distance, and assign that centroid&rsquo;s label</strong>.&rdquo; The proof of this is within your grasp!</p>
<p><strong>M-Step</strong>: Coming up with parameters, based on full assignments. If you work out the math of chosing the best parameter values based on the features of a given piece of data in your data set, it comes out to &ldquo;<strong>take the mean of all the data-points that were labeled as c</strong>.&rdquo;</p>
<h1 id="how-to-choose-k">how to choose k</h1>
<p>the loss should continue to decrease for larger numbers of centers (the more centers, the closer any given point will be to them). But unlike supervised learning, there is not even a good analogue of cross-validation that we can use here: this property of lower loss will typically also apply to a validation set as well.</p>
<p>A common strategy is rather just to plot the loss versus the number of clusters and try to find a point that is “good enough” in terms of loss versus the number of cluster (i.e., where adding additional clusters doesn’t help much).</p>
<p>it is very difficult to infer anything about the “real” number of clusters in the data from running k-means (in fact, you should really never try to do this)</p>
<ul>
<li>Heuristic: Find large gap between k -1-means cost and kmeans cost.</li>
<li>Hold-out validation/cross-validation on auxiliary task (e.g.,supervised learning task).</li>
<li>Try hierarchical clustering</li>
</ul>

  </div>
  

<div class="navigation navigation-single">
    
    <a href="http://www.tczhong.com/posts/machinelearning/validation/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">Cross-validation</span>
    </a>
    
    
    <a href="http://www.tczhong.com/posts/mac/ntfs/" class="navigation-next">
      <span class="navigation-tittle">How to mount NTFS in Mac</span>
      <i aria-hidden="true" class="fa fa-chevron-right"></i>
    </a>
    
</div>


  

  
    
        <div id="disqus_thread"></div>
<script type="text/javascript">
    

    (function () {
    if (location.hostname === "localhost" ||
      location.hostname === "127.0.0.1" ||
      location.hostname === "") {
      return;
    }
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    var disqus_shortname = 'tczhong';
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || 
      document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>

<noscript>
  Please enable JavaScript to view the
  <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by
  <span class="logo-disqus">Disqus</span>
</a>

    


</article>


        </div>
        
    

<script defer src="https://use.fontawesome.com/releases/v5.11.2/js/all.js" integrity="sha384-b3ua1l97aVGAPEIe48b4TC60WUQbQaGi2jqAWM90y0OZXZeyaTCWtBTKtjW2GXG1" crossorigin="anonymous"></script>


    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
        
    <script type="text/javascript">
        
        hljs.initHighlightingOnLoad();
    </script>
    



    



    </body>
</html>
