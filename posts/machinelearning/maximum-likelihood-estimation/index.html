<!DOCTYPE html>
<html lang="en" >
<head>
  <meta charset="utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>

  
  <meta name="author"
        content=""/>

  
  <meta name="description" content="Basic idea for Maximum likelihood estimation"/>
  

  

  
  <link rel="canonical" href="http://www.tczhong.com/posts/machinelearning/maximum-likelihood-estimation/"/>

  

  <title>Maximum likelihood estimation &middot; TC blog</title>

  <link rel="shortcut icon" href="http://www.tczhong.com/images/favicon.ico"/>
  <link rel="stylesheet" href="http://www.tczhong.com/css/animate.min.css"/>
  <link rel="stylesheet" href="http://www.tczhong.com/css/remixicon.css"/>
  <link rel="stylesheet" href="http://www.tczhong.com/css/zozo.css"/>
  <link rel="stylesheet" href="http://www.tczhong.com/css/highlight.css"/>

  
  
</head>

<body>
<div class="main animated">
  <div class="nav_container animated fadeInDown">
  <div class="site_nav" id="site_nav">
    <ul>
      
      <li>
        <a href="http://www.tczhong.com/posts/">Posts</a>
      </li>
      
      <li>
        <a href="http://www.tczhong.com/about/hello">About</a>
      </li>
      
      <li>
        <a href="http://www.tczhong.com/tianchen_zhong.pdf">My Resume</a>
      </li>
      
      <li>
        <a href="http://www.tczhong.com/posts/project/index.html">Project</a>
      </li>
      
    </ul>
  </div>
  <div class="menu_icon">
    <a id="menu_icon"><i class="remixicon-links-line"></i></a>
  </div>
</div>

  <div class="header animated fadeInDown">
  <div class="site_title_container">
    <div class="site_title">
      <h1>
        <a href="http://www.tczhong.com/">
          <span>TC blog</span>
          <img src="http://www.tczhong.com/img/kirby.png"/>
        </a>
      </h1>
    </div>
    <div class="description">
      <p class="sub_title"></p>
      <div class="my_socials">
        
        <a href="http://www.tczhong.com/index.xml" type="application/rss+xml" title="rss" target="_blank"><i class="remixicon-rss-fill"></i></a>
      </div>
    </div>
  </div>
</div>

  <div class="content">
    <div class="post_page">
      <div class="post animated fadeInDown">
        <div class="post_title post_detail_title">
          <h2><a href='http://www.tczhong.com/posts/machinelearning/maximum-likelihood-estimation/'>Maximum likelihood estimation</a></h2>
          <span class="date">2018.05.01</span>
        </div>
        <div class="post_content markdown"><h1 id="maximum-likelihood-estimation">Maximum likelihood estimation</h1>
<p>In the beginning, we could get a sample set $x_1,x_2,&hellip;$, and we assume these points are independent with each other. Then we could get the possibility for us to get these points.
$p(x_1,x_2,x_3&hellip;,x_m;\theta) = \prod_{i=1}^m p(x_i;\theta)$</p>
<p>So, what&rsquo;s the most possible $\theta$ under this circumstance, it is when this possibility get its maximum value. Therefore, we want to maximize $\theta$, to calculate
$$maximize_\theta ; \prod_{i=1}^m p(x^{(i)};\theta)$$
If we take the log to $p(x)$, we could get the **log likelihood** of the data, which is also equivalent to that value.
$$maximize_\theta ; \frac{1}{m} \sum_{i=1}^m \log p(x^{(i)};\theta)$$</p>
<p>Therefore, when we need to calculate $\phi$ in Bernoulli distribution, we could use $p(X = x; \phi) = \phi^x (1-\phi)^{(1-x)}$ to capture $x=1$ and $x=0$ at the same time. Then we could rewrite it to $maximize_{\phi} \sum_{i=1}^m (x^{(i)}\log\phi + (1-x^{(i)}) \log (1-\phi))$</p>
<p>In order to maximize this equation, let&rsquo;s take the derivative and set it equal to 0. So, we could know why we could estimate $\phi$ just like this.
$$\begin{split}
\frac{d}{d \phi} \sum_{i=1}^m \left (x^{(i)}\log\phi + (1-x^{(i)}) \log (1-\phi) \right )
&amp; = \sum_{i=1}^m \frac{d}{d \phi} \left ( x^{(i)}\log\phi + (1-x^{(i)}) \log (1-\phi) \right )  \<br>
&amp; = \sum_{i=1}^m \left ( \frac{x^{(i)}}{\phi} - \frac{1-x^{(i)}}{1-\phi} \right )
\end{split}$$</p>
<p>$$\begin{split}
&amp; \sum_{i=1}^m \left ( \frac{x^{(i)}}{\phi} - \frac{1-x^{(i)}}{1-\phi} \right ) = 0 \<br>
\Longrightarrow ;; &amp; \frac{\sum_{i=1}^m x^{(i)}}{\phi} - \frac{\sum_{i=1}^m (1-x^{(i)})} {1-\phi} = 0 \<br>
\Longrightarrow ;; &amp; (1-\phi) \sum_{i=1}^m x^{(i)}  - \phi \sum_{i=1}^m (1-x^{(i)}) = 0 \<br>
\Longrightarrow ;; &amp; \sum_{i=1}^m x^{(i)} = \phi \sum_{i=1}^m (x^{(i)} + (1-x^{(i)})) \<br>
\Longrightarrow ;; &amp; \sum_{i=1}^m x^{(i)} = \phi m \<br>
\Longrightarrow ;; &amp; \phi = \frac{\sum_{i=1}^m x^{(i)}}{m}.
\end{split}$$</p>
<h2 id="naive-bayes">Naive bayes</h2>
<p>Naive bayes is an easy method, the key is to calculate $$p(Y \mid X) = \frac{p(X \mid Y)p(Y)}{\sum_y p(X \mid y) p(y)}$$. We could calculate $P(X \ mid Y)$ and $P(Y)$ respectively.</p>
<p>The naive Bayes approach, however, is to make the additional assumption that the individual features  $X_i$  are conditionally independent given  $Y$ . This lets us represent the condition $p(X \mid Y) = \prod_{i=1}^n p(X_i \mid Y)$.</p>
<p>For example, we need to detect spam mail. We need to training the classifier. The training data tells us $P(words|spam)$ and $P(words'|not spam)$, then we could get the $P(spam)$, we could use $P(spam|words&rsquo;') = \frac{p(words|spam)P(spam)}{p(words|spam)P(spam)+p(words|not spam)P(not spam)}$</p>
<p>We could use log to avoid too small value yield from $\prod$, like $\log p(y) + \sum_{i=1}^n \log p(x_i \mid y)$.</p>
<p>We also could use laplace smoothing to avoid zero. $\phi_i^y = \frac{\sum_{j=1}^m x_i^{(j)} \mathrm{1}{y^{(j)} = y} + 1}{\sum_{j=1}^m \mathrm{1}{y^{(j)} = y} + 2}$</p>
<h2 id="logistic-regression">logistic regression</h2>
<p>We could get this formula from logistic regression and we also could think it using MLE.</p>
<p>$minimize_\theta \sum_{i=1}^m \ell_{\mathrm{logistic}}(h_\theta(x^{(i)}),y^{(i)}) ; \equiv ; minimize_\theta ;; \sum_{i=1}^m \log(1+\exp(-h_\theta(x^{(i)})\cdot y^{(i)})$</p>
</div>
        <div class="post_footer">
          
        </div>
      </div>
      
      
    </div>
  </div>
  <a id="back_to_top" href="#" class="back_to_top"><span>â–³</span></a>
</div>
<footer class="footer">
  <div class="powered_by">
    <a href="https://zeuk.me">Designed by Zeuk,</a>
    <a href="http://www.gohugo.io/">Proudly published with Hugo</a>
  </div>

  <div class="footer_slogan">
    <span></span>
  </div>
</footer>



<script src="http://www.tczhong.com/js/jquery-3.3.1.min.js"></script>
<script src="http://www.tczhong.com/js/zozo.js"></script>
<script src="http://www.tczhong.com/js/highlight.pack.js"></script>
<link  href="http://www.tczhong.com/css/fancybox.min.css" rel="stylesheet">
<script src="http://www.tczhong.com/js/fancybox.min.js"></script>

<script>hljs.initHighlightingOnLoad()</script>





<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "tczhong" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

</body>
</html>
