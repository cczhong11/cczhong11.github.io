<!DOCTYPE html>
<html lang="en" class="wf-firasans-n4-active wf-active">
	<head>
    <link href="http://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <!-- Enable responsiveness on mobile devices -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
    
    
    <meta name="generator" content="Hugo 0.37" />
    <title></title>
    <meta content="" property="og:title">
    <meta content="" property="og:description">
    <!-- CSS -->
    <link href="https://fonts.googleapis.com/css?family=Fira+Sans:300,300i,400,400i|Roboto+Mono:300,300i,400,400i" rel="stylesheet">
    <link rel="stylesheet" href="https://cczhong11.github.io/css/print.css" media="print">
    <link rel="stylesheet" href="https://cczhong11.github.io/css/poole.css">
    <link rel="stylesheet" href="https://cczhong11.github.io/css/hyde.css">
    <!-- Font-Awesome -->
    <script defer src="https://use.fontawesome.com/releases/v5.0.10/js/all.js" integrity="sha384-slN8GvtUJGnv6ca26v8EzVaR9DC58QEwsIk9q1QXdCU8Yu8ck/tL/5szYlBbqmS+" crossorigin="anonymous"></script>
    <!-- highlight.js-->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/vs.min.css">
    <!-- Customised CSS -->
    <link rel="stylesheet" href="https://cczhong11.github.io/css/custom.css">
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://cczhong11.github.io/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="https://cczhong11.github.io/favicon.png">
    <!-- RSS -->
    <link href="https://cczhong11.github.io/index.xml" rel="alternate" type="application/rss+xml" title="" />
    <link href="https://cczhong11.github.io/index.xml" rel="feed" type="application/rss+xml" title="" />
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
    (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-1279783853013476",
        enable_page_level_ads: true
    });
	</script>
	</head>
    <body>
        <div class="sidebar">
	<div class="container text-center sidebar-sticky">
		<div class="sidebar-about text-center">
			<a href="https://cczhong11.github.io/"><h1 class="brand"></h1></a>
			 <img src="https://cczhong11.github.io/img/kirby.png" alt="Author Image" class="img-circle headshot center"> 
			<p class="lead">
				 I am who I am. Love Cloud and Data science 
			</p>
		</div>
		
<div>
	<ul class="sidebar-nav">
		
		
				<li>
					<a href="https://cczhong11.github.io/posts/"> <span>Posts</span></a>
				</li>
				<li>
					<a href="https://cczhong11.github.io/about/"> <span>About</span></a>
				</li>
				<li>
					<a href="https://cczhong11.github.io/resume/static/tianchen_zhong.pdf"> <span>Resume</span></a>
				</li>
		</li>
	</ul>
</div>

        <p>
		<section class="row text-center">
	
	
	
	&nbsp;<a href="https://github.com/cczhong11"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	&nbsp;<a href="https://linkedin.com/in/tianchen-zhong"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	&nbsp;<a href="mailto:tczhong24@gmail.com"><i class="fas fa-at fa-lg" aria-hidden="true"></i></a>
	
</section>

        </p>
		<p class="copyright">&copy; 2018 Tc Zhong.
        <a href="https://creativecommons.org/licenses/by/4.0">Some Rights Reserved</a>.<br/>Built with <a href="https://gohugo.io/">Hugo</a> &amp; <a href="https://github.com/htr3n/hyde-hyde">hyde-hyde</a>.
        </p>
	</div>
	<div>
	</div>
</div>

        <div class="content container">
            <div class="posts">
    
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/resume/a/">Resume</a>
        </h1>
        <span class="post-date">Tue, Jun 5, 2018</span>
        resume 
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/mac/graphvis/">How to draw graph in code</a>
        </h1>
        <span class="post-date">Thu, May 24, 2018</span>
        Basic Drawing We could use Graphviz to draw a graph in code and it is also easily to used in Markdown file. In this blog, I will focus on dot tool.
In the beginning, we need to install it. If you are a Mac user, we could use brew install graphvis. For other user, you could download file from the official website.
You could create a file with the following code. 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/mac/graphvis/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/machinelearning/anomaly-and-multivariate-gaussia-distribution/">Anomaly and Mixture of Gaussian and </a>
        </h1>
        <span class="post-date">Thu, May 10, 2018</span>
        What is Anomaly In supervised view, we could think these points are with some labels. In unsupervised view, there are outliers, which means they have very low probability.
When we could model a dataset with a distribution, we could find which point&rsquo;s probability is very low, then we could think them as a outlier.
Multivariate Gaussian distribution $$P(x) = \frac{1}{\Sigma \sqrt {2\pi}} e^{-({x - \mu } )^2/{2\Sigma ^2}}$$
In this distribution, we could prove $E[X] = \mu$, $Cov[X]=\Sigma$ 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/machinelearning/anomaly-and-multivariate-gaussia-distribution/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/machinelearning/decision-tree/">Decision Tree</a>
        </h1>
        <span class="post-date">Tue, May 8, 2018</span>
        what is desicion tree Decision tree is tree-based algorithm and it has two type: regression tree and classification tree.
The main idea for decision tree is to choose one place to cut the whole sample space at one time to get two branches and cut recurisively. In ID3 algorithm, we only consider one demension to cut the space. You can think of the hypothesis function of decision trees as partitioning the input space with axis-aligned boundaries. 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/machinelearning/decision-tree/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/machinelearning/dimension_reduction/"> Dimension Rerduction</a>
        </h1>
        <span class="post-date">Sat, May 5, 2018</span>
        Dimension Rerduction PCA Principal component analysis (PCA) looks at “simplifying” the data in another manner, by preserving the axes of major variation in the data.
Hypothesis function $h_\theta (x) = UWx, \theta={U \in R^{n*k},W\in R^{k*n}}$. We are compressing input by multiplying by a low rank matriex.
Loss function $$\ell(h_\theta(x), x)=|\mu - x|_2^2$$
Optimization problem. $$minimize{U,W} = \sum{I=1}^m | UWx^{(i)}-x^{(i)}|^2_2$$
procedure Given: normalized data matrix X, k components
 Compute singular value decomposition $USV^T = X$, where $U,V$ is orthogonal and $S$ is diagonal matrix of singular values. 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/machinelearning/dimension_reduction/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/mac/ntfs/">How to mount NTFS in Mac</a>
        </h1>
        <span class="post-date">Sat, May 5, 2018</span>
        Install FUSE FUSE for macOS allows you to extend macOS&rsquo;s native file handling capabilities via third-party file systems. You could install it here.
Install ntfs-3g Use
brew install ntfs-3g  If you run into error, Error: Could not symlink sbin/mkntfs. You need to use the absolute path to run the following function.
Next, we need to umount our driver, since it only has read permission. And create an endpoint for it. 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/mac/ntfs/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/machinelearning/cluster/">Clustering</a>
        </h1>
        <span class="post-date">Tue, May 1, 2018</span>
        unsupervised learning the general philosophy of unsupervised learning is that we want to discover some kind of structure in the data. Different unsupervised learning methods work in very different ways, and discover very different kinds of structure, but they all have this similar element.
Recall from our presentations on supervised learnign that the three aspects of a supervised learning algorithm are: 1. a hypothesis function;($R^n -&gt; R^n$). Mapping input back to this input space. 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/machinelearning/cluster/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/machinelearning/validation/">Cross-validation</a>
        </h1>
        <span class="post-date">Tue, May 1, 2018</span>
        Cross-validation Hold - out validation In this method, we will have 70% training data and 30% validation data. When we do the training, we will only use the training data, then we will use the validation data to test our model and tuning the hyperparameters
 parameters:$\theta$ hypermeters: degree of polynominal, amount of regulazation&hellip;  K-hold validation In this method, we will divide the data into k part, then we will use k-1 data to train the model and have the rest one validate the model. 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/machinelearning/validation/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/machinelearning/hypothesis-testing/">Hypothesis testing</a>
        </h1>
        <span class="post-date">Tue, May 1, 2018</span>
        Hypothesis testing sample statistics and central limit theorem When we have a dataset, we could sample data from this set. And we could use sample result to estimate statistics for total set. For example, in total we have 10000 data points, but we could not get them all. Therefore, we sampled 100 data points to guess the original dataset expectation and variance. We also could know the sample mean&rsquo;s expectation and variance. 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/machinelearning/hypothesis-testing/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/machinelearning/logistic-regression/">Logistic regression</a>
        </h1>
        <span class="post-date">Tue, May 1, 2018</span>
        Logistic regression This method is a varanice of a linear classier. The key this is we want to use $sigmod(\theta X)$ to classifer value.
Gradient descent This method like linear regression, it needs Least square minimization for error. Because there is only 0 and 1. Therefore we could use probability to identify the error. $g(z) = \frac{1}{1+e^{-z}}$
$P(y=0|x,\theta) = sigmod(\theta X)$
$P(y=1|x,\theta) = 1-sigmod(\theta X)$
这里使用最大似然估计，目的是得到在已知样本的情况下最有可能的$\theta$ 的值。
$L(\theta) = \prod p(y^i|x^i;\theta)$ 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/machinelearning/logistic-regression/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/machinelearning/maximum-likelihood-estimation/">Maximum likelihood estimation</a>
        </h1>
        <span class="post-date">Tue, May 1, 2018</span>
        Maximum likelihood estimation In the beginning, we could get a sample set $x_1,x_2,&hellip;$, and we assume these points are independent with each other. Then we could get the possibility for us to get these points. $p(x_1,x_2,x_3&hellip;,xm;\theta) = \prod{i=1}^m p(x_i;\theta)$
So, what&rsquo;s the most possible $\theta$ under this circumstance, it is when this possibility get its maximum value. Therefore, we want to maximize $\theta$, to calculate $$maximize\theta \; \prod{i=1}^m p(x^{(i)};\theta)$$ If we take the log to $p(x)$, we could get the log likelihood of the data, which is also equivalent to that value. 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/machinelearning/maximum-likelihood-estimation/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/machinelearning/neuralnetwork/">Neural network</a>
        </h1>
        <span class="post-date">Tue, May 1, 2018</span>
        Neural network The key difference is it combines non-linear function with linear function. It could implement any function by using enough neural.
Before, we could learn $h_\theta(x)=\theta^T\phi(x)$, if we still set $\phi(x)$ to be a linear function, which we think it should be a two-stage function, but in the end it is still a one stage linear function.
Hypothesis function Neural networks are a simple extension of this idea, where we additionally apply a non-linear function after each linear transformation. 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/machinelearning/neuralnetwork/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/machinelearning/probaility/">Probability</a>
        </h1>
        <span class="post-date">Tue, May 1, 2018</span>
        Probability Here we want to talk about some basic knowledges of probability. In the beginning, we need to understand some notations in probability and they are confused in some times.
$p(x),p_x$ here, $x$ is an exact number and we could have an exact number for this value from 0 to 1. For exmaple, $p(1)=0.5,p(0)=0.5$
$p(X)$ here, $X$ is a random variable, it represents all the possible value. We could use $p(X=1)$ to represent the exact value. 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/machinelearning/probaility/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/machinelearning/probability_modeling/">Probability modeling</a>
        </h1>
        <span class="post-date">Tue, May 1, 2018</span>
        Probability modeling What is probability modeling? It is a high dimension distribution for p(X). It represents distribution more compactly by exploiting conditional independencies.
Bayesian Network The main idea is to represent $P(X)$ with $\prod P(X_i|parents(X_i))$. It is also a generative model, which means it construct distribution as a &lsquo;sequential story&rsquo;.
Markov chain Monte Carlo Markov chain Monte Carlo (MCMC) refers to a class of methods that approximately draw samples from over the hidden variables 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/machinelearning/probability_modeling/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/machinelearning/recommender/">Recommendation</a>
        </h1>
        <span class="post-date">Tue, May 1, 2018</span>
        Recommendation Collaborative filtering Solely upon the preferences that other users have indicated for these items.(rating for items): row -&gt; users column-&gt; items
The task of collaborative filtering, then, is to “fill in” the remaining entries of this matrix given the observed matrix. This X matrix that we observed is sparse but the unknown entries do not correspond to actual zeros in the matrix, but are rather just truly unknown. 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/machinelearning/recommender/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/cc/undertow/">Undertow configuration</a>
        </h1>
        <span class="post-date">Fri, Apr 27, 2018</span>
        Undertow This is a very fast Java web server. When using this web server, I met some problems but I could not find enough answers even on the stackOverflow. Therefore, I decided to write my experience down.
How to set parameters to each Servlet? It is necessary for us to pass some parameters to Serlet functions. One way to do this is to use addInitParam when adding Servlet. Like the following code. 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/cc/undertow/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/cc/scala-spark-join/">Scala join RDD tips</a>
        </h1>
        <span class="post-date">Sat, Apr 21, 2018</span>
        Why Spark RDD join is expensive As we know, Join is the most expensive operation on rdd. The reason is when we join two rdd, it requires corresponding keys from each RDD are located at the same partition so that they can be combined locally. Therefore, we will shuffle keys from each rdd to make sure they are in the same partition.
You could learn more about rdd join from here 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/cc/scala-spark-join/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/cc/hbase-reading/">Improve performance on HBase</a>
        </h1>
        <span class="post-date">Tue, Apr 17, 2018</span>
        Tips on improving reading performance In some scenario, we need to have high RPS in HBase reading.Therefore, we could change some configuration in HBase to meet this requirement. I found this blog very helpful. HBase official guide is also great.
 Increase hfile.block.cache.size and decrease hbase.regionserver.global.memstore.size. The first configuration is for reading cache and the second one is writing cache. The sum of two value should be 0.8 Decrease BLOCKSIZE in HBase table. 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/cc/hbase-reading/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/cc/mapreduce/">Mapreduce custom output</a>
        </h1>
        <span class="post-date">Tue, Apr 10, 2018</span>
        Introduction Mapreduce is a very strong computing framework and we could use it to transform data to HDFS. Acutally, we could use Mapreduce to write data to any databse.
Basic input and output The key thing in Mapreduce is Mapper and Reducer. We use the following code snippets to build a mapper. Here, Mapper&lt;Object, Text, Text, Text&gt; means input key and value class and ouput key and value class. Please notice that we could not use string and int here, since it is not fit the requirement in HDFS. 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/cc/mapreduce/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/cc/gcp-spark/">GCP for Spark</a>
        </h1>
        <span class="post-date">Tue, Mar 27, 2018</span>
        GCP is very cheap compared with AWS and Azure. However, it did not have enough documentation and I met some problems that did not even on StackOverflow. That&rsquo;s why I want to write this blog.
Start your cluster We could use command line with GCP CLI to start cluster.
gcloud dataproc clusters create cluster-name --project=project-id --bucket outputbucket --initialization-actions gs://xxx/jupyter.sh --master-machine-type=n1-standard-2 --worker-machine-type=n1-standard-1 --zone=xxxx  Install external Python package on Spark If we want to use your own package and Juypter NoteBook on Spark, you need to contain an initialization step for your cluster. 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/cc/gcp-spark/">Read More…</a>
        </div>
        
    </div>
    <div class="post">
        <h1 class="post-title">
            <a href="https://cczhong11.github.io/posts/cc/spark-etl/">Spark for ETL</a>
        </h1>
        <span class="post-date">Tue, Mar 27, 2018</span>
        There is not much information about how could we perform complex operation on pySpark. I summary some tips from stackoverflow and my experience. Hope it will be helpful for you.
How to Read JSON to DataFrame There are serveral ways to read json in Spark. The most common way is to load json in a DataFrame. Why not rdd? Because json contains some hierachy information and it could not represent well in RDD. 
        <div class="read-more-link">
            <a href="https://cczhong11.github.io/posts/cc/spark-etl/">Read More…</a>
        </div>
        
    </div>
</div>
            <div class="footer">
                
            </div>
        </div>
        
                
    </body>
</html>
