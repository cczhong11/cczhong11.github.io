<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gcp on </title>
    <link>https://cczhong11.github.io/categories/gcp/</link>
    <description>Recent content in Gcp on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 08 Jul 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://cczhong11.github.io/categories/gcp/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Leetcode Answer websites</title>
      <link>https://cczhong11.github.io/project/leetcodeanswer/</link>
      <pubDate>Sun, 08 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://cczhong11.github.io/project/leetcodeanswer/</guid>
      <description>The idea It is popular for students to practise their coding skills on leetcode.com. It offers very good questions and good articles. However, it is not easy for people to learn from other good answers. Although it provides discussion, but people needs to go through threads by threads to find the right answers. I want to build a website for myself to find questions easily.
All codes are belong to Leetcode.</description>
    </item>
    
    <item>
      <title>GCP for Spark</title>
      <link>https://cczhong11.github.io/posts/cc/gcp-spark/</link>
      <pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://cczhong11.github.io/posts/cc/gcp-spark/</guid>
      <description>GCP is very cheap compared with AWS and Azure. However, it did not have enough documentation and I met some problems that did not even on StackOverflow. That&amp;rsquo;s why I want to write this blog.
Start your cluster We could use command line with GCP CLI to start cluster.
gcloud dataproc clusters create cluster-name --project=project-id --bucket outputbucket --initialization-actions gs://xxx/jupyter.sh --master-machine-type=n1-standard-2 --worker-machine-type=n1-standard-1 --zone=xxxx  Install external Python package on Spark If we want to use your own package and Juypter NoteBook on Spark, you need to contain an initialization step for your cluster.</description>
    </item>
    
  </channel>
</rss>