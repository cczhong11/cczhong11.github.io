<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ml on TC blog</title>
    <link>http://www.tczhong.com/categories/ml/</link>
    <description>Recent content in Ml on TC blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 12 Dec 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://www.tczhong.com/categories/ml/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Machine learning basic concepts</title>
      <link>http://www.tczhong.com/posts/ml/machinelearningreview/</link>
      <pubDate>Wed, 12 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.tczhong.com/posts/ml/machinelearningreview/</guid>
      <description>Deep Neural Networks Neural Networks In general, neural networks have lots of node and each node will perform a transformation. In the easiest way, a linear transmission could be viewed as $\sum w _ i x _ i$. It is the hard bias for this neural network, it is how each neuron could do. Neural Networks are also built as a layered Feedforward network, and the hard bias is the topology and activation function, they represent the expressive power for the network.</description>
    </item>
    
    <item>
      <title>Maximum Likelihood Estimation basic concepts</title>
      <link>http://www.tczhong.com/posts/ml/mle/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.tczhong.com/posts/ml/mle/</guid>
      <description>Maximum Likelihood Estimation This is a fundamental algorithm, but I still want to write a blog about it. Let&amp;rsquo;s start with the basic idea.
Probability If we have a situation $S_1$ and event A may happen. For example, under the same situation, the event A happens 7 out of 10. We could see the frequency of event A is 0.7. Under much experiment with the same situation, if we still get the same result, we could see the probability of event A in this situation is 0.</description>
    </item>
    
    <item>
      <title>Decision tree basic concepts</title>
      <link>http://www.tczhong.com/posts/ml/decisiontree/</link>
      <pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.tczhong.com/posts/ml/decisiontree/</guid>
      <description>Decision tree Decision tree is an essential algorithm in Machine Learning field. Using Decision tree can express all possible functions among the data. Understanding the decision tree requires us to know entropy in the first place.
Entropy Consider we flip an unbiased coin, and we can get head and tail both in 50% chance. However, if we flip a biased coin, we can get head in 90% chance and tail 10% chance.</description>
    </item>
    
    <item>
      <title>K Nearest Neighbor, K-Means and EM algorithm</title>
      <link>http://www.tczhong.com/posts/ml/knn/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.tczhong.com/posts/ml/knn/</guid>
      <description>K Nearest Neighbor, K-Means and EM algorithm These three algorithms are used for clustering. Let&amp;rsquo;s talk about K Nearest Neighbor(KNN) first.
K Nearest Neighbor Example of k-NN classification. The test sample (green circle) should be classified either to the first class of blue squares or to the second class of red triangles. If k = 3 (solid line circle) it is assigned to the second class because there are 2 triangles and only 1 square inside the inner circle.</description>
    </item>
    
  </channel>
</rss>