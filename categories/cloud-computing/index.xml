<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cloud Computing on TC blog</title>
    <link>http://www.tczhong.com/categories/cloud-computing/</link>
    <description>Recent content in Cloud Computing on TC blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 15 Jul 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://www.tczhong.com/categories/cloud-computing/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Solve TimeZone in Azure Web App</title>
      <link>http://www.tczhong.com/posts/cc/azure-timezone/</link>
      <pubDate>Sun, 15 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.tczhong.com/posts/cc/azure-timezone/</guid>
      <description>TimeZone&amp;rsquo;s Problem In my intern project, I need to handle timezone problem. Because in US, there is Daylight time saving and 5 diffferent timezone. My code should provide service for people in different timezones. Therefore, in the front end, I could allow user input time in their own timezone, but I should save date and time in GMT+0. Then I could make sure all time in database is saved in the same timezone.</description>
    </item>
    
    <item>
      <title>Solve NoSuchMethodError in Azure JAVA SDK </title>
      <link>http://www.tczhong.com/posts/cc/azure-error/</link>
      <pubDate>Thu, 05 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.tczhong.com/posts/cc/azure-error/</guid>
      <description>Problem I met a NoSuchMethodError in Azure Java SDK. The error message is like the following:
Exception in thread &amp;quot;main&amp;quot; java.lang.NoSuchMethodError: com.microsoft.azure.credentials.ApplicationTokenCredentials.proxy()Ljava/net/Proxy;  It was very strange and I searched on the stackoverflow. And it shows the problem could be confliction in package version. Therefore I looked into the code to find the reason.
Solution In the beginning, I needed to go into the source code to see what happened there.</description>
    </item>
    
    <item>
      <title>Create a thumbnail of photo using Java</title>
      <link>http://www.tczhong.com/posts/java/createthumbnail/</link>
      <pubDate>Fri, 22 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.tczhong.com/posts/java/createthumbnail/</guid>
      <description>Create a thumbnail of photo using Java Why we need a thumbnail In the mobile application, if we want to show a picture quickly and we do not care the clarity of that image. We could create a thumbnail of that image, which is a smaller version of this image. The original image captured by iPhone X could be 10 MB, but the thumbnail only have 50 kb, which is much more handy to use when you need to show them quickly.</description>
    </item>
    
    <item>
      <title>Spring MVC model</title>
      <link>http://www.tczhong.com/posts/java/spring_mvc/</link>
      <pubDate>Thu, 21 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.tczhong.com/posts/java/spring_mvc/</guid>
      <description>Spring boot Framework Spring boot framework is a strong framework helps you build a MVC model web service quickly. So in the first place, we need to understand what is MVC model
Model-View-Controller  Model  Model represent knowledge. It represents how basic items interact with each other.
 Views  A view is a (visual) representation of its model. A view is attached to its model (or model part) and gets the data necessary for the presentation from the model by asking questions.</description>
    </item>
    
    <item>
      <title>Undertow configuration</title>
      <link>http://www.tczhong.com/posts/cc/undertow/</link>
      <pubDate>Fri, 27 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.tczhong.com/posts/cc/undertow/</guid>
      <description>Undertow This is a very fast Java web server. When using this web server, I met some problems but I could not find enough answers even on the stackOverflow. Therefore, I decided to write my experience down.
How to set parameters to each Servlet? It is necessary for us to pass some parameters to Serlet functions. One way to do this is to use addInitParam when adding Servlet. Like the following code.</description>
    </item>
    
    <item>
      <title>Scala join RDD tips</title>
      <link>http://www.tczhong.com/posts/cc/scala-spark-join/</link>
      <pubDate>Sat, 21 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.tczhong.com/posts/cc/scala-spark-join/</guid>
      <description>Why Spark RDD join is expensive As we know, Join is the most expensive operation on rdd. The reason is when we join two rdd, it requires corresponding keys from each RDD are located at the same partition so that they can be combined locally. Therefore, we will shuffle keys from each rdd to make sure they are in the same partition.
You could learn more about rdd join from here</description>
    </item>
    
    <item>
      <title>Improve performance on HBase</title>
      <link>http://www.tczhong.com/posts/cc/hbase-reading/</link>
      <pubDate>Tue, 17 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.tczhong.com/posts/cc/hbase-reading/</guid>
      <description>Tips on improving reading performance In some scenario, we need to have high RPS in HBase reading.Therefore, we could change some configuration in HBase to meet this requirement. I found this blog very helpful. HBase official guide is also great.
 Increase hfile.block.cache.size and decrease hbase.regionserver.global.memstore.size. The first configuration is for reading cache and the second one is writing cache. The sum of two value should be 0.8 Decrease BLOCKSIZE in HBase table.</description>
    </item>
    
    <item>
      <title>Mapreduce custom output</title>
      <link>http://www.tczhong.com/posts/cc/mapreduce/</link>
      <pubDate>Tue, 10 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.tczhong.com/posts/cc/mapreduce/</guid>
      <description>Introduction Mapreduce is a very strong computing framework and we could use it to transform data to HDFS. Acutally, we could use Mapreduce to write data to any databse.
Basic input and output The key thing in Mapreduce is Mapper and Reducer. We use the following code snippets to build a mapper. Here, Mapper&amp;lt;Object, Text, Text, Text&amp;gt; means input key and value class and ouput key and value class. Please notice that we could not use string and int here, since it is not fit the requirement in HDFS.</description>
    </item>
    
    <item>
      <title>GCP for Spark</title>
      <link>http://www.tczhong.com/posts/cc/gcp-spark/</link>
      <pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.tczhong.com/posts/cc/gcp-spark/</guid>
      <description>GCP is very cheap compared with AWS and Azure. However, it did not have enough documentation and I met some problems that did not even on StackOverflow. That&amp;rsquo;s why I want to write this blog.
Start your cluster We could use command line with GCP CLI to start cluster.
gcloud dataproc clusters create cluster-name --project=project-id --bucket outputbucket --initialization-actions gs://xxx/jupyter.sh --master-machine-type=n1-standard-2 --worker-machine-type=n1-standard-1 --zone=xxxx  Install external Python package on Spark If we want to use your own package and Juypter NoteBook on Spark, you need to contain an initialization step for your cluster.</description>
    </item>
    
    <item>
      <title>Spark for ETL</title>
      <link>http://www.tczhong.com/posts/cc/spark-etl/</link>
      <pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.tczhong.com/posts/cc/spark-etl/</guid>
      <description>There is not much information about how could we perform complex operation on pySpark. I summary some tips from stackoverflow and my experience. Hope it will be helpful for you.
How to Read JSON to DataFrame There are serveral ways to read json in Spark. The most common way is to load json in a DataFrame. Why not rdd? Because json contains some hierachy information and it could not represent well in RDD.</description>
    </item>
    
  </channel>
</rss>