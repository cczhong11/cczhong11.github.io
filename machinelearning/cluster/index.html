<!DOCTYPE html>
<html lang="en" class="wf-firasans-n4-active wf-active">
	<head>
    <link href="http://gmpg.org/xfn/11" rel="profile">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge"> <!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">   <meta name="generator" content="Hugo 0.47" /> 
<title>Clustering &middot; TC blog</title>
<meta content="Clustering - TC blog"
    property="og:title">
<meta content=" - This blog tells you do cluster"
    property="og:description"> <!-- CSS -->
<link href="https://fonts.googleapis.com/css?family=Fira+Sans:300,300i,400,400i|Roboto+Mono:300,300i,400,400i" rel="stylesheet">
<link rel="stylesheet" href="http://www.tczhong.com/css/print.css" media="print">
<link rel="stylesheet" href="http://www.tczhong.com/css/poole.css">
<link rel="stylesheet" href="http://www.tczhong.com/css/hyde.css"> <!-- Font-Awesome -->
<script defer src="https://use.fontawesome.com/releases/v5.0.10/js/all.js" integrity="sha384-slN8GvtUJGnv6ca26v8EzVaR9DC58QEwsIk9q1QXdCU8Yu8ck/tL/5szYlBbqmS+"
    crossorigin="anonymous"></script> <!-- highlight.js-->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/vs.min.css"> <!-- Customised CSS -->
<link rel="stylesheet" href="http://www.tczhong.com/css/custom.css"> <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]--> <!-- Icons -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://www.tczhong.com/apple-touch-icon-144-precomposed.png">
<link rel="shortcut icon" href="http://www.tczhong.com/favicon.png"> 

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-120924697-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-120924697-1');
</script>


	</head>
    <body>
        <div class="sidebar">
	<div class="container text-center sidebar-sticky">
		<div class="sidebar-about text-center">
			<a href="http://www.tczhong.com/"><h1 class="brand">TC blog</h1>
			 <img src="https://avatars2.githubusercontent.com/u/7358252?s=460&amp;v=4" alt="Author Image" class="img-circle headshot center"> </a>
			<p class="lead">
				 I am who I am. Love Cloud and Data science 
			</p>
		</div>
		
<div>
	<ul class="sidebar-nav">
		
		
				<li>
					<a href="http://www.tczhong.com/posts/"> <span>Posts</span></a>
				</li>
				<li>
					<a href="http://www.tczhong.com/about/hello"> <span>About</span></a>
				</li>
				<li>
					<a href="http://www.tczhong.com/tianchen_zhong.pdf"> <span>My Resume</span></a>
				</li>
				<li>
					<a href="http://www.tczhong.com/posts/project/index.html"> <span>Project</span></a>
				</li>
		</li>
	</ul>
</div>

        <p>
		<section class="row text-center">
	
	
	
	&nbsp;<a href="https://github.com/cczhong11"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	&nbsp;<a href="https://linkedin.com/in/tianchen-zhong"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	&nbsp;<a href="https://stackoverflow.com/users/7112540"><i class="fab fa-stack-overflow fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	&nbsp;<a href="mailto:tczhong24@gmail.com"><i class="fas fa-at fa-lg" aria-hidden="true"></i></a>
	
</section>

        </p>
		<p class="copyright">&copy; 2019 Tc Zhong.
        <a href="https://creativecommons.org/licenses/by/4.0">Some Rights Reserved</a>.<br/>Built with <a href="https://gohugo.io/">Hugo</a> &amp; <a href="https://github.com/htr3n/hyde-hyde">hyde-hyde</a>.
        </p>
	</div>
	<div>
	</div>
</div>

        <div class="content container">
            <div class="post">
  <h1>Clustering</h1>
  
  <div class="col-sm-12 col-md-12">
    <span class="text-left post-date meta">
            
       <i class="fa fa-calendar" aria-hidden="true"></i> May 5, 2018 
      
      
        
        
            in
            
            
                <a class="meta" href="http://www.tczhong.com/categories/machine-learning">MACHINE LEARNING</a>
                
            
        
      
      
      
      </span>  
  </div>    
  
  

<h1 id="unsupervised-learning">unsupervised learning</h1>

<p>the general philosophy of unsupervised learning is that we want to discover some kind of structure in the data. Different unsupervised learning methods work in very different ways, and discover very different kinds of structure, but they all have this similar element.</p>

<p>Recall from our presentations on supervised learnign that the three aspects of a supervised learning algorithm are:
1. a hypothesis function;($R^n -&gt; R^n$). Mapping input back to this input <strong>space.The goal of this hypothesis class is to approximately reconstruct the input</strong>
2. a loss function; the goal of an unsupervised learning loss function is to measure the <strong>difference between the hypothesis function and the input itself</strong>.
3. a method for minimizing the average loss over the training data. in the unsupervised setting there (just empirically) is a bit more <strong>variation</strong> in the methods people use to solve this optimization problem. This is because the hypothesis functions themselves often involve discrete terms or other similar elements that cannot be differentiated (we’ll see this is the case of k-means clustering), or because particular optimization methods can provide exact solution</p>

<h1 id="kmeans">kmeans</h1>

<h2 id="hypothesis-function">Hypothesis function</h2>

<p>The parameters $\theta$ of our hypothesis function just include the centers themselves.
$$\theta = {\mu^{(1)}, \ldots, \mu^{(k)}}$$</p>

<p>The function is $$h<em>\theta(x) = argmin</em>{\mu \in {\mu^{(1)}, \ldots, \mu^{(k)}} } |\mu - x|_2^2$$</p>

<h2 id="loss-function">Loss function</h2>

<p>The loss function used by k-means is simply the squared loss
$$\ell(h<em>\theta(x), x) = \min</em>{\mu \in {\mu^{(1)}, \ldots, \mu^{(k)}}} |\mu - x|_2^2$$</p>

<h2 id="optimization">Optimization</h2>

<p>Finally, let&rsquo;s now consider the ML optimization problem that results from the hypothesis and loss above,
$$minimize<em>\theta \;\; \frac{1}{m} \sum</em>{i=1}^m \min_{\mu \in {\mu^{(1)}, \ldots, \mu^{(k)}}} |\mu - x^{(i)}|_2^2.$$</p>

<ol>
<li>choose centre randomly</li>
<li>repeat until convergence</li>
<li>for every i, set $c^{(i)}=argmin ||x^{(i)}-\mu_j||^2$ $c^{(i)}$ is label for each point</li>
<li>for every $\mu_j$ set $\mu_j=\frac{\sum (c(i)=j)x^{i}}{\sum (c(i)=j)}$ Each centroid is the geometric mean of the points that have that centroid&rsquo;s label. Important: If a centroid is empty</li>
</ol>

<pre><code class="language-python">def kmeans(X, k, max_iter=10, rand_seed=0):
    np.random.seed(rand_seed)
    Mu = X[np.random.choice(X.shape[0],k),:]
    for i in range(max_iter):
        D = -2*X@Mu.T + (X**2).sum(axis=1)[:,None] + (Mu**2).sum(axis=1)
        y = np.argmin(D,axis=1)
        Mu = np.array([np.mean(X[y==i],axis=0) for i in range(k)])
    loss = np.linalg.norm(X - Mu[np.argmin(D,axis=1),:])**2/X.shape[0]
    return Mu, y, loss
</code></pre>

<p>The problem for k-means is it is possible to get into local optima.</p>

<h1 id="expectation-maximization">Expectation Maximization</h1>

<p>the EM algorithm gives an efficient method for maximum likelihood estimation. Maximizing ℓ(θ) explicitly might be difficult, and our strategy will be to instead repeatedly construct a lower-bound on l(E-step), and then optimize that lower-bound (M-step).</p>

<p>Lets review EM. In EM, you randomly initialize your model parameters, then you alternate between (E) assigning values to hidden variables, based on parameters and (M) computing parameters based on fully observed data.</p>

<p><strong>E-Step</strong>: Coming up with values to hidden variables, based on parameters. If you work out the math of chosing the best values for the class variable based on the features of a given piece of data in your data set, it comes out to &ldquo;<strong>for each data-point, chose the centroid that it is closest to, by euclidean distance, and assign that centroid&rsquo;s label</strong>.&rdquo; The proof of this is within your grasp!</p>

<p><strong>M-Step</strong>: Coming up with parameters, based on full assignments. If you work out the math of chosing the best parameter values based on the features of a given piece of data in your data set, it comes out to &ldquo;<strong>take the mean of all the data-points that were labeled as c</strong>.&rdquo;</p>

<h1 id="how-to-choose-k">how to choose k</h1>

<p>the loss should continue to decrease for larger numbers of centers (the more centers, the closer any given point will be to them). But unlike supervised learning, there is not even a good analogue of cross-validation that we can use here: this property of lower loss will typically also apply to a validation set as well.</p>

<p>A common strategy is rather just to plot the loss versus the number of clusters and try to find a point that is “good enough” in terms of loss versus the number of cluster (i.e., where adding additional clusters doesn’t help much).</p>

<p>it is very difficult to infer anything about the “real” number of clusters in the data from running k-means (in fact, you should really never try to do this)</p>

<ul>
<li>Heuristic: Find large gap between k -1-means cost and kmeans cost.</li>
<li>Hold-out validation/cross-validation on auxiliary task (e.g.,supervised learning task).</li>
<li>Try hierarchical clustering</li>
</ul>

</div>
            <div class="footer">
                <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>

<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>


        <h2>Comments</h2>
        <div id="disqus_thread"></div>
<script type="text/javascript">
      (function () {
            
            
            
            if (location.hostname === "localhost" || 
            	location.hostname === "127.0.0.1" || 
            	location.hostname === "") {
                return;
			}
            var dsq = document.createElement('script');
            dsq.type = 'text/javascript';
            dsq.async = true;
            var disqus_shortname = 'tczhong';
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(
                  dsq);
      })();
</script>

<noscript>
	Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    

            </div>
        </div>
        
                
    </body>
</html>
