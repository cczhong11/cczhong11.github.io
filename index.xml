<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>https://cczhong11.github.io/</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 27 Mar 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://cczhong11.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>GCP for Spark</title>
      <link>https://cczhong11.github.io/posts/cc/gcp-spark/</link>
      <pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://cczhong11.github.io/posts/cc/gcp-spark/</guid>
      <description>GCP is very cheap compared with AWS and Azure. However, it did not have enough documentation and I met some problems that did not even on StackOverflow. That&amp;rsquo;s why I want to write this blog.
Start your cluster We could use command line with GCP CLI to start cluster.
gcloud dataproc clusters create cluster-name --project=project-id --bucket outputbucket --initialization-actions gs://xxx/jupyter.sh --master-machine-type=n1-standard-2 --worker-machine-type=n1-standard-1 --zone=xxxx  Install external Python package on Spark If we want to use your own package and Juypter NoteBook on Spark, you need to contain an initialization step for your cluster.</description>
    </item>
    
    <item>
      <title>My first blog</title>
      <link>https://cczhong11.github.io/about/hello/</link>
      <pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://cczhong11.github.io/about/hello/</guid>
      <description>Hello World This is my first blog and I want to express something I learned in my graduate year. CMU is a very great place and I could meet many clever and hard-working students in this place. I learn the latest technology and skills in this school, and I also gain many precious experiences here.
Spend time wisely I believe I have a good time management skill and I could handle 4 courses in CMU in one semester.</description>
    </item>
    
    <item>
      <title>Spark for ETL</title>
      <link>https://cczhong11.github.io/posts/cc/spark-etl/</link>
      <pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://cczhong11.github.io/posts/cc/spark-etl/</guid>
      <description>There is not much information about how could we perform complex operation on pySpark. I summary some tips from stackoverflow and my experience. Hope it will be helpful for you.
How to Read JSON to DataFrame There are serveral ways to read json in Spark. The most common way is to load json in a DataFrame. Why not rdd? Because json contains some hierachy information and it could not represent well in RDD.</description>
    </item>
    
  </channel>
</rss>